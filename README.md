# NeuralDelphi

<p align="center">
  <img src="https://img.shields.io/badge/Delphi-EE1F35?style=for-the-badge&logo=delphi&logoColor=white" alt="Delphi"/>
  <img src="https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge" alt="MIT License"/>
  <img src="https://img.shields.io/badge/Platform-Windows-0078D6?style=for-the-badge" alt="Windows"/>
</p>

**A high-performance, pure Delphi machine learning framework.** No Python. No external DLLs. Just fast, native code.

---

## âœ¨ Features

- **ğŸš€ Arena-Based Memory** â€” Zero allocation/deallocation during training
- **âš¡ SIMD Assembly** â€” Hand-tuned SSE kernels for x64
- **ğŸ”„ Automatic Differentiation** â€” Full autograd with computation graphs
- **ğŸ§µ Thread Pool Parallelization** â€” Efficient multi-core utilization
- **ğŸ“¦ Zero Dependencies** â€” Pure Delphi, compiles standalone

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        NeuralDelphi                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ML.Arena    â”‚  Linear memory allocator (zero GC overhead)       â”‚
â”‚  ML.Tensor   â”‚  Lightweight tensor views into arena              â”‚
â”‚  ML.Ops      â”‚  SIMD kernels + parallel operations               â”‚
â”‚  ML.Graph    â”‚  Computation graph + autograd                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PERSISTENT PARAMS             â”‚  TEMPORARY ACTIVATIONS  â”‚
â”‚  (weights, biases, gradients)         â”‚  (reset each iteration) â”‚
â”‚                                       â”‚                         â”‚
â”‚  â† MarkParamsEnd()                    â”‚  â† ResetActivations()   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ XOR Demo

The included demo trains a neural network to learn the XOR function in real-time:

| Input A | Input B | Expected Output |
|---------|---------|-----------------|
| 0       | 0       | 0 (Red)         |
| 0       | 1       | 1 (Blue)        |
| 1       | 0       | 1 (Blue)        |
| 1       | 1       | 0 (Red)         |

**Network Architecture:**
```
Input(2) â†’ Dense(8) â†’ LeakyReLU â†’ Dense(1) â†’ Sigmoid â†’ Output
```

## ğŸ”§ Building

### Requirements
- **RAD Studio** 11+ (Delphi)
- **Platform:** Windows x64 (for SIMD assembly)

### Steps
1. Open `XOR_Demo.dpr` in RAD Studio
2. Select **64-bit Windows** target
3. Build and Run (F9)

> **Note:** 32-bit builds use scalar fallbacks (no SIMD)

## ğŸ“ Project Structure

```
NeuralDelphi/
â”œâ”€â”€ ML.Arena.pas      # Memory arena allocator
â”œâ”€â”€ ML.Tensor.pas     # Tensor record (view into arena)
â”œâ”€â”€ ML.Ops.pas        # Math operations + SIMD kernels
â”œâ”€â”€ ML.Graph.pas      # Computation graph + autograd
â”œâ”€â”€ XOR_Demo.dpr      # Interactive XOR visualization
â”œâ”€â”€ LICENSE           # MIT License
â””â”€â”€ README.md
```

## ğŸ§  Core Concepts

### Arena Allocation
Traditional allocators are too slow for ML training loops. The arena pre-allocates a contiguous memory block:

```delphi
Arena := TArena.Create(256);  // 256MB block
Ptr := Arena.Alloc(1000);     // O(1) allocation
Arena.Reset;                   // O(1) free everything
```

### Computation Graph
Operations are recorded on a "tape" for automatic differentiation:

```delphi
W := Graph.Param(8, 2);       // Trainable weights
X := Graph.Input(2, 1);       // Input placeholder
H := Graph.MatMul(W, X);      // Forward: H = W @ X
A := Graph.LeakyReLU(H);      // Forward: A = LeakyReLU(H)

Graph.Backward(LossNode);      // Backward: compute all gradients
Graph.Step(0.01);              // Update: W -= lr * dW
```

### SIMD Kernels
Critical operations use hand-written x64 assembly:

```delphi
// SSE dot product - 4 floats at once
class function TKernels.DotProduct(const PtrA, PtrB: PSingle; K: Integer): Single;
asm
  XORPS XMM7, XMM7       // Accumulator = 0
@Loop:
  MOVUPS XMM0, [RAX]     // Load 4 floats from A
  MOVUPS XMM1, [RCX]     // Load 4 floats from B
  MULPS  XMM0, XMM1      // Multiply packed
  ADDPS  XMM7, XMM0      // Accumulate
  ...
end;
```

## ğŸ“Š Supported Operations

| Category | Operations |
|----------|------------|
| **Core** | MatMul, Add, Mul |
| **Activations** | ReLU, LeakyReLU, Sigmoid, Tanh, Softmax |
| **Loss** | MSE, CrossEntropy, SoftmaxCrossEntropy |

## ğŸš§ Roadmap

- [ ] Model save/load persistence
- [ ] Batch training support
- [ ] Conv2D operations
- [ ] MNIST demo
- [ ] AVX-512 kernels
- [ ] GPU acceleration (CUDA/OpenCL)

## ğŸ¤ Contributing

Contributions welcome! Areas of interest:
- Additional layer types
- Performance optimizations
- More demos and examples
- Documentation

## ğŸ“œ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

<p align="center">
  <i>Built with â¤ï¸ in Delphi</i>
</p>

