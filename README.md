# NeuralDelphi

<p align="center">
  <img src="https://img.shields.io/badge/Delphi-EE1F35?style=for-the-badge&logo=delphi&logoColor=white" alt="Delphi"/>
  <img src="https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge" alt="MIT License"/>
  <img src="https://img.shields.io/badge/Platform-Windows-0078D6?style=for-the-badge" alt="Windows"/>
</p>

**A high-performance, pure Delphi machine learning framework.** No Python. No external DLLs. Just fast, native code.

---

## ‚ú® Features

- **üöÄ Arena-Based Memory** ‚Äî Zero allocation/deallocation during training
- **‚ö° SIMD Assembly** ‚Äî Hand-tuned SSE kernels for x64
- **üîÑ Automatic Differentiation** ‚Äî Full autograd with computation graphs
- **üßµ Thread Pool Parallelization** ‚Äî Efficient multi-core utilization
- **üì¶ Zero Dependencies** ‚Äî Pure Delphi, compiles standalone

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        NeuralDelphi                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ML.Arena    ‚îÇ  Linear memory allocator (zero GC overhead)       ‚îÇ
‚îÇ  ML.Tensor   ‚îÇ  Lightweight tensor views into arena              ‚îÇ
‚îÇ  ML.Ops      ‚îÇ  SIMD kernels + parallel operations               ‚îÇ
‚îÇ  ML.Graph    ‚îÇ  Computation graph + autograd                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Details

#### **ML.Arena.pas** - Memory Management
The foundation of NeuralDelphi's performance. Implements a **linear allocator** (also called a "bump allocator" or "arena allocator") that pre-allocates a large contiguous block of memory.

**Key Concepts:**
- **`TMemPtr`**: An `Integer` index into the arena, not a pointer. This avoids pointer arithmetic issues and makes the system 32/64-bit agnostic.
- **`TArena.Alloc(Count)`**: O(1) allocation - just increments the head pointer. No free lists, no fragmentation.
- **`TArena.Reset()`**: O(1) deallocation - sets head to 0. All memory is "freed" instantly.
- **`GetSavePoint()` / `Restore()`**: Critical for the graph architecture. Allows resetting only temporary activations while keeping persistent parameters.

**Why This Matters:**
Traditional `GetMem`/`FreeMem` calls are expensive (kernel calls, heap fragmentation). During training, you might allocate millions of temporary tensors. The arena eliminates this overhead entirely.

**Example:**
```delphi
Arena := TArena.Create(256);        // Allocate 256MB block
W1 := Arena.Alloc(8 * 2);          // Allocate 16 floats (8x2 matrix)
W2 := Arena.Alloc(1 * 8);          // Allocate 8 floats (1x8 matrix)
// ... use W1, W2 ...
Arena.Reset;                        // Free everything instantly
```

---

#### **ML.Tensor.pas** - Tensor Abstraction
A lightweight `record` (not a class!) that acts as a **view** into the arena. Think of it like a pointer + metadata.

**Key Fields:**
- **`DataPtr: TMemPtr`**: Index into arena where tensor data lives
- **`GradPtr: TMemPtr`**: Index for gradients (allocated on-demand during backward pass)
- **`Rows, Cols: Integer`**: Shape information
- **`RequiresGrad: Boolean`**: Whether this tensor needs gradients computed

**Key Methods:**
- **`RawData(Arena)`**: Returns `PSingle` pointer for direct memory access (used by SIMD kernels)
- **`RawGrad(Arena)`**: Returns gradient pointer, or `nil` if not allocated
- **`CreateTensor()`**: Factory method that allocates memory in arena and returns a tensor view

**Why Records, Not Classes:**
- Zero heap allocation overhead
- Value semantics (can copy freely)
- Cache-friendly (all data in one contiguous block)

**Example:**
```delphi
var
  T: TTensor;
begin
  T := TTensor.CreateTensor(Arena, 8, 2, True);  // 8x2 matrix, needs gradients
  // T.DataPtr now points to 16 floats in the arena
  // T.GradPtr = -1 (not allocated yet)
end;
```

---

#### **ML.Ops.pas** - Mathematical Operations
Contains three layers: **Pure ASM Kernels**, **Parallel Execution**, and **High-Level Tensor Ops**.

**1. `TKernels` - Pure Assembly Math Kernels**
Hand-written x64 SSE assembly for maximum performance. These are **stateless** functions that operate on raw pointers.

- **`DotProduct(A, B, Count)`**: SIMD dot product using `MOVUPS`, `MULPS`, `ADDPS`, `HADDPS`. Processes 4 floats at once.
- **`VectorAdd(A, B, Out, Count)`**: Element-wise addition with SSE `ADDPS`.
- **`VectorMul(A, B, Out, Count)`**: Element-wise multiplication with SSE `MULPS`.
- **`Transpose(Src, Dst, Rows, Cols)`**: Block-based matrix transpose (8x8 blocks) for cache efficiency.

**Why Separate Kernels:**
- Can't use inline ASM inside anonymous methods (Delphi limitation)
- Kernels are reusable across different operations
- Easy to optimize independently

**2. `TMLParallel` - Thread Pool Wrapper**
Wraps `System.Threading.TParallel.For` with a threshold check. Only parallelizes if workload is substantial (>256 elements) to avoid overhead.

**3. `TOps` - High-Level Tensor Operations**
Combines kernels + parallelism + tensor management. Each operation:
- Validates tensor shapes
- Allocates output tensor in arena
- Calls appropriate kernels (SIMD or scalar)
- Parallelizes outer loops when beneficial

**Key Operations:**
- **`MatMul`**: Matrix multiplication. Transposes B for cache-friendly access, parallelizes rows, uses SIMD dot product for inner loop.
- **`Add` / `Mul`**: Element-wise operations using SIMD kernels.
- **`ReLU` / `LeakyReLU` / `Sigmoid`**: Activation functions (scalar, but could be SIMD-optimized).
- **`MSE` / `CrossEntropy`**: Loss functions.
- **`*Backward`**: Gradient computation for each operation (chain rule).

**Example:**
```delphi
// Forward pass
TOps.MatMul(Arena, W, X, Out);        // Out = W @ X (uses SIMD + parallel)
TOps.LeakyReLU(Arena, Out, Activated); // Activated = LeakyReLU(Out)

// Backward pass
TOps.MatMulBackward(Arena, W, X, OutGrad, WGrad, XGrad);  // Computes dW, dX
```

---

#### **ML.Graph.pas** - Computation Graph & Autograd
The "brain" of NeuralDelphi. Implements automatic differentiation by building a computation graph.

**Key Concepts:**

**1. Computation Graph:**
Each operation creates a `TNode` that records:
- Operation type (`opMatMul`, `opReLU`, etc.)
- Input node indices (parents)
- Output tensor
- Whether gradients are needed

**2. Forward Pass:**
Operations are executed immediately as you build the graph:
```delphi
W := Graph.Param(8, 2);        // Creates param node, allocates memory
X := Graph.Input(2, 1);         // Creates input placeholder
H := Graph.MatMul(W, X);       // Executes MatMul, creates node
A := Graph.LeakyReLU(H);       // Executes LeakyReLU, creates node
```

**3. Backward Pass:**
Traverses graph in reverse, computing gradients using chain rule:
```delphi
Graph.Backward(LossNode);  // Computes gradients for all nodes requiring them
```

**4. Memory Architecture:**
- **`MarkParamsEnd()`**: Called after all `Param()` calls. Marks the boundary between persistent parameters and temporary activations.
- **`ResetActivations()`**: Resets arena to param savepoint. Wipes activations but keeps parameters intact. This is the key optimization that eliminates save/restore overhead.

**Key Methods:**
- **`Param(Rows, Cols)`**: Creates trainable parameter. Pre-allocates gradients so they persist across `ResetActivations()`.
- **`Input(Rows, Cols)`**: Creates input placeholder (value set later via `SetInputValue()`).
- **`MatMul(A, B)`**: Creates matrix multiplication node, executes forward pass.
- **`Backward(LossNode)`**: Computes gradients for all nodes that need them.
- **`Step(LearningRate)`**: Updates parameters: `W -= lr * dW`.

**Example:**
```delphi
// Build network (once)
W := Graph.Param(8, 2);
B := Graph.Param(8, 1);
Graph.MarkParamsEnd();  // Mark: everything before this is persistent

// Training loop
for i := 1 to 1000 do
begin
  Graph.ResetActivations();  // Wipe activations, keep W and B
  X := Graph.Input(2, 1);
  H := Graph.MatMul(W, X);
  H := Graph.Add(H, B);
  Y := Graph.LeakyReLU(H);
  Loss := Graph.MSE(Y, Target);
  
  Graph.ZeroGrad();      // Zero param gradients
  Graph.Backward(Loss);  // Compute gradients
  Graph.Step(0.01);      // Update: W -= 0.01 * dW
end;
```

### Memory Model

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PERSISTENT PARAMS             ‚îÇ  TEMPORARY ACTIVATIONS  ‚îÇ
‚îÇ  (weights, biases, gradients)         ‚îÇ  (reset each iteration) ‚îÇ
‚îÇ                                       ‚îÇ                         ‚îÇ
‚îÇ  ‚Üê MarkParamsEnd()                    ‚îÇ  ‚Üê ResetActivations()   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üéØ XOR Demo

The included demo (`XOR_Demo.dpr`) trains a neural network to learn the XOR function in real-time with a visual heatmap.

### The XOR Problem

XOR (exclusive OR) is a classic non-linearly separable problem that requires a hidden layer:

| Input A | Input B | Expected Output | Visual |
|---------|---------|-----------------|--------|
| 0       | 0       | 0               | Red    |
| 0       | 1       | 1               | Blue   |
| 1       | 0       | 1               | Blue   |
| 1       | 1       | 0               | Red    |

**Why XOR is Hard:**
- A single-layer perceptron cannot learn XOR (it's not linearly separable)
- Requires at least one hidden layer with non-linear activation
- Tests that the network can learn non-linear decision boundaries

### Network Architecture

```
Input(2) ‚Üí MatMul(W1: 8x2) ‚Üí Add(B1: 8x1) ‚Üí LeakyReLU ‚Üí 
          MatMul(W2: 1x8) ‚Üí Add(B2: 1x1) ‚Üí Sigmoid ‚Üí Output(1)
```

**Layer Breakdown:**
- **Input Layer**: 2 neurons (XOR inputs)
- **Hidden Layer**: 8 neurons with LeakyReLU activation
  - `W1`: 8√ó2 weight matrix (16 parameters)
  - `B1`: 8√ó1 bias vector (8 parameters)
- **Output Layer**: 1 neuron with Sigmoid activation (probability)
  - `W2`: 1√ó8 weight matrix (8 parameters)
  - `B2`: 1√ó1 bias scalar (1 parameter)
- **Total Parameters**: 33 trainable weights/biases

**Training:**
- **Learning Rate**: 0.5 (higher for faster convergence on small dataset)
- **Loss Function**: MSE (Mean Squared Error)
- **Optimizer**: SGD (Stochastic Gradient Descent) - `W -= lr * dW`
- **Dataset**: 4 samples (all XOR combinations), repeated each epoch

**Visualization:**
- Heatmap shows network's prediction for every (x, y) coordinate
- Red = predicts 0, Blue = predicts 1
- Corners show actual XOR truth table
- Updates in real-time as network learns

## üîß Building

### Requirements
- **RAD Studio** 11+ (Delphi)
- **Platform:** Windows x64 (for SIMD assembly)

### Steps
1. Open `XOR_Demo.dpr` in RAD Studio
2. Select **64-bit Windows** target
3. Build and Run (F9)

> **Note:** 32-bit builds use scalar fallbacks (no SIMD)

## üìÅ Project Structure

```
NeuralDelphi/
‚îú‚îÄ‚îÄ ML.Arena.pas      # Memory arena allocator
‚îú‚îÄ‚îÄ ML.Tensor.pas     # Tensor record (view into arena)
‚îú‚îÄ‚îÄ ML.Ops.pas        # Math operations + SIMD kernels
‚îú‚îÄ‚îÄ ML.Graph.pas      # Computation graph + autograd
‚îú‚îÄ‚îÄ XOR_Demo.dpr      # Interactive XOR visualization
‚îú‚îÄ‚îÄ LICENSE           # MIT License
‚îî‚îÄ‚îÄ README.md
```

## üß† How It All Works Together

### Training Step Flow

Here's what happens during a single training iteration:

```
1. ResetActivations()
   ‚îî‚îÄ> Arena.Restore(ParamSavePoint)
       ‚îî‚îÄ> Wipes temporary tensors, keeps W, B, dW, dB

2. Build Forward Pass
   ‚îî‚îÄ> Graph.Input()     ‚Üí Allocates input tensor
   ‚îî‚îÄ> Graph.MatMul()    ‚Üí Calls TOps.MatMul()
       ‚îî‚îÄ> Transposes B for cache efficiency
       ‚îî‚îÄ> Parallel.ForEach(row) ‚Üí TKernels.DotProduct() (SIMD)
   ‚îî‚îÄ> Graph.LeakyReLU() ‚Üí Element-wise activation
   ‚îî‚îÄ> Graph.MSE()       ‚Üí Computes loss

3. Backward Pass
   ‚îî‚îÄ> Graph.ZeroGrad()  ‚Üí Zeros param gradients (they persist!)
   ‚îî‚îÄ> Graph.Backward()  ‚Üí Traverses graph in reverse
       ‚îî‚îÄ> For each node: calls TOps.*Backward()
           ‚îî‚îÄ> Uses chain rule: dA = dOut * dOut/dA
           ‚îî‚îÄ> Accumulates gradients: dW += gradient

4. Parameter Update
   ‚îî‚îÄ> Graph.Step(lr)    ‚Üí W -= lr * dW, B -= lr * dB
```

### Memory Layout Example

After `MarkParamsEnd()`, the arena looks like:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Offset ‚îÇ Size ‚îÇ Content                                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0      ‚îÇ 16   ‚îÇ W1[8x2] data                                ‚îÇ
‚îÇ 16     ‚îÇ 8    ‚îÇ B1[8x1] data                                ‚îÇ
‚îÇ 24     ‚îÇ 8    ‚îÇ W2[1x8] data                                ‚îÇ
‚îÇ 32     ‚îÇ 1    ‚îÇ B2[1x1] data                                ‚îÇ
‚îÇ 33     ‚îÇ 16   ‚îÇ W1 gradients (pre-allocated)                ‚îÇ
‚îÇ 49     ‚îÇ 8    ‚îÇ B1 gradients                                ‚îÇ
‚îÇ 57     ‚îÇ 8    ‚îÇ W2 gradients                                ‚îÇ
‚îÇ 65     ‚îÇ 1    ‚îÇ B2 gradients                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 66     ‚îÇ ‚Üê FParamSavePoint (MarkParamsEnd() saved this)      ‚îÇ
‚îÇ        ‚îÇ      ‚îÇ                                              ‚îÇ
‚îÇ        ‚îÇ      ‚îÇ ‚Üê ResetActivations() restores to here        ‚îÇ
‚îÇ        ‚îÇ      ‚îÇ                                              ‚îÇ
‚îÇ 66+    ‚îÇ 2    ‚îÇ Input[2x1] (temporary)                      ‚îÇ
‚îÇ 68+    ‚îÇ 8    ‚îÇ Hidden[8x1] (temporary)                      ‚îÇ
‚îÇ 76+    ‚îÇ 1    ‚îÇ Output[1x1] (temporary)                     ‚îÇ
‚îÇ 77+    ‚îÇ 1    ‚îÇ Loss[1x1] (temporary)                       ‚îÇ
‚îÇ        ‚îÇ      ‚îÇ ... gradients for activations ...            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Performance Optimizations

**1. SIMD (Single Instruction, Multiple Data)**
- Processes 4 floats simultaneously using SSE registers
- `DotProduct`: ~4x faster than scalar code
- `VectorAdd`/`VectorMul`: ~4x faster for element-wise ops

**2. Cache-Friendly Matrix Multiplication**
- Transposes matrix B before multiplication
- Accesses memory sequentially (row-major)
- Reduces cache misses by ~80% vs naive implementation

**3. Thread Pool Parallelization**
- Uses Delphi RTL's `TParallel.For` (reuses threads)
- Parallelizes outer loops (rows of output matrix)
- Threshold: only parallelizes if >256 elements (avoids overhead)

**4. Zero-Copy Operations**
- Tensors are views, not copies
- Operations write directly to arena
- No intermediate allocations

**5. Persistent Parameters**
- Parameters allocated once, gradients pre-allocated
- `ResetActivations()` only wipes temporary tensors
- Eliminates save/restore overhead (was ~100 array operations per iteration)

## üìä Supported Operations

### Core Operations

| Operation | Description | Forward | Backward |
|-----------|-------------|---------|----------|
| **MatMul** | Matrix multiplication `C = A @ B` | SIMD dot product, parallel rows, transposed B for cache | `dA = dC @ B^T`, `dB = A^T @ dC` |
| **Add** | Element-wise addition `C = A + B` | SIMD `ADDPS` | `dA = dC`, `dB = dC` (broadcast) |
| **Mul** | Element-wise multiplication `C = A * B` | SIMD `MULPS` | `dA = dC * B`, `dB = dC * A` |

### Activation Functions

| Operation | Formula | Use Case |
|-----------|---------|----------|
| **ReLU** | `f(x) = max(0, x)` | Standard activation, fast, can cause "dying ReLU" |
| **LeakyReLU** | `f(x) = max(Œ±x, x)` where Œ±=0.01 | Prevents dying ReLU, allows small negative gradients |
| **Sigmoid** | `f(x) = 1/(1+e^(-x))` | Output layer for binary classification, smooth gradient |
| **Tanh** | `f(x) = tanh(x)` | Centered around 0, stronger gradients than sigmoid |
| **Softmax** | `f(x_i) = e^(x_i) / Œ£e^(x_j)` | Multi-class classification, outputs probability distribution |

### Loss Functions

| Operation | Formula | Use Case |
|-----------|---------|----------|
| **MSE** | `L = (1/n)Œ£(pred - target)¬≤` | Regression tasks, smooth gradients |
| **CrossEntropy** | `L = -Œ£(target * log(pred))` | Classification, but numerically unstable |
| **SoftmaxCrossEntropy** | Combined softmax + cross-entropy | **Recommended** for classification. Numerically stable, simple gradient: `pred - target` |

### Backward Operations

All operations have corresponding `*Backward` methods that compute gradients using the **chain rule**:

- **Chain Rule**: If `y = f(x)` and `z = g(y)`, then `dz/dx = dz/dy * dy/dx`
- **Accumulation**: Gradients accumulate (add) when a tensor is used in multiple operations
- **Lazy Allocation**: Gradients are only allocated when `RequiresGrad = True` and `Backward()` is called

## üéì Design Decisions & Trade-offs

### Why Records Instead of Classes?
- **Zero heap allocation**: Records are value types, stored on stack or inline
- **Cache-friendly**: All tensor metadata in one small struct
- **No vtable overhead**: Direct method calls, no virtual dispatch
- **Trade-off**: Can't use inheritance, but we don't need it

### Why Arena Instead of Standard Allocator?
- **Speed**: O(1) allocation vs O(log n) for heap allocators
- **No fragmentation**: Contiguous memory, perfect for SIMD
- **Predictable performance**: No GC pauses, no heap walks
- **Trade-off**: Can't free individual tensors (but we don't need to in training loops)

### Why Inline Assembly Instead of Compiler Intrinsics?
- **Delphi's SIMD support is limited**: No direct access to SSE intrinsics like C++
- **Full control**: We can optimize exactly how we want
- **Portability**: x64 gets SIMD, x86 gets scalar fallback (same code path)
- **Trade-off**: Platform-specific, but ML frameworks are typically platform-specific anyway

### Why Separate Kernels from Operations?
- **Delphi limitation**: Can't use inline ASM inside anonymous methods
- **Reusability**: Kernels can be called from anywhere
- **Testability**: Can unit test kernels independently
- **Trade-off**: Slight indirection, but negligible performance impact

### Why ResetActivations Instead of Full Reset?
- **Performance**: Eliminates ~100 array copy operations per iteration
- **Memory efficiency**: Parameters stay in place, no save/restore
- **Simplicity**: No need to track parameter arrays separately
- **Trade-off**: Slightly more complex arena management, but worth it

### Why Pre-allocate Parameter Gradients?
- **Persistence**: Gradients must survive `ResetActivations()`
- **Performance**: Allocate once, reuse forever
- **Simplicity**: No need to check if gradient exists during backward pass
- **Trade-off**: Uses more memory upfront, but negligible for typical networks

## üöß Roadmap

- [ ] Model save/load persistence
- [ ] Batch training support
- [ ] Conv2D operations
- [ ] MNIST demo
- [ ] AVX-512 kernels
- [ ] GPU acceleration (CUDA/OpenCL)

## ü§ù Contributing

Contributions welcome! Areas of interest:
- Additional layer types
- Performance optimizations
- More demos and examples
- Documentation

## üìú License

MIT License ‚Äî see [LICENSE](LICENSE) for details.

---

<p align="center">
  <i>Built with ‚ù§Ô∏è in Delphi</i>
</p>

